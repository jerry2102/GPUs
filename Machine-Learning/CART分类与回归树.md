
# CART
CART(Classification and Regression Tree, CART)是一种经典的决策树，可以用来处理涉及连续数据的分类或者回归任务。


# GBDT
GBDT：Gradient Boosting Decision Tree，梯度提升决策树。

# 集成学习
ensemble learning，通过构建并结合多个学习器来完成学习任务。继承学习一般县产生一组个体学习器（通常使用弱学习器），在用某种策略将他们结合起来。根据个体学习器的生成方式。  
目前的集成学习方法大致可以分为两类：1)个体学习器间存在强依赖关系，必须串行生成的序列化集成方法，典型代表是Boosting；2)个体学习器间不存在强依赖关系，可以同时生成的并行化方法，典型代表是Bagging。

stacking：将个体学习器称为初级学习器，用于结合的学习器称为次级学习器或者元学习器（meta-learner），stacking县从出时数据及训练出初级学习器，再生成一个新数据及用于训练次级学习器。

[1] [解读集成树模型-AdaBoost/GBDT/XGB/LightGBM/CatBoost](万字长文解读集成树模型——AdaBoost/GBDT/XGB/LGBM/CatBoost/RandomForest)



# 参数化模型和非参数化模型
## 参数模型
参数模型通常**假设样本服从某个分布**，这个分布可以由一组有限且固定数目的参数所表示，在此基础上构建的模型称为参数模型，即参数化模型拟合的是给定特征下，输出的分布情况（参数指模型的参数）。
如线性回归是常见的参数模型，它假设输入变量和输出变量之间有线性关系$y=w^Tx + b$，通过合适的方法比如最小二乘法来拟合目标函数的参数$w$。
在参数化模型中，有限的参数能够描述无限的数据的原因是：**超强的先验假设**，即假设所有数据符合特定类型的概率分布，数据是独立同分布的。由于假设了数据满足特定的先验分布，所以学习过程就是以利用训练数据估计位置参数的过程。

### 参数化模型的优缺点
- **优点**：简单，可解释性强；学习快，时和数据量少的场景
- **缺点**：函数约束；适合复杂度低的问题；容易欠拟合


 
## 非参数化模型
非参数模型**不对数据的分布做假设**，即不能固定数量的参数来刻画数据分布。非参数模型不是说模型没有参数，而是说模型不能通过固定且有限数目的参数来完全刻画。如KNN算法，每一个训练数据都可以视为一个参数，故模型参数是不固定的。  
通常说到的机器学习黑盒特性，一般指的就是非参数机器学习模型。因为它不需要作出假设，且需要拟合很多参数，所以非参数模型的解释性低；而参数化模型由于对数据作出了理想的假设，所得到的模型解释性非常高

### 非参数化模型优缺点
- **优点**：可变性：可以拟合许多不同的函数形式；能力强；表现良好
- **缺点**：需要更多数据；训练慢；训练数据过拟合风险更大

常见模型
| 模型 | 类别 | 说明 | 
| --- | --- | --- |
|感知机| 参数化模型 | $$f(x)=sign(w^Tx + b)$$| 
|逻辑回归(LR, Logistic Regression)|参数化模型| |
|线性支持向量机||
​
 
 
# 常见机器学习算法

## 参数化模型

### 感知机
[1] [感知机算法](https://mp.weixin.qq.com/s?__biz=MzUyODk0Njc1NQ==&mid=2247486944&idx=1&sn=14db60a9f9300605374493a0b70eaa14&chksm=fa69cecccd1e47da07f875eaac07b8212c313db6c77e295187637d0545ef5cee27c6f8a921f6&scene=21#wechat_redirect)

### 线性可分支持向量机

[1] [线性可分支持向量机](https://mp.weixin.qq.com/s?__biz=MzUyODk0Njc1NQ==&mid=2247487215&idx=1&sn=ee5d57895f0cbf0f1b7163ffb0eae651&chksm=fb27ecd89a34e7436ce9e469eeac98a0b7e31082e5f9d1139e5d90ac879836ee23c6619ffdb1&scene=27)

### 简单神经网络
--

### k均值聚类
通过迭代的方式将数据及分为K个聚类，每个聚类由一个中心点（或称质心）表示。算法优化目标是最小化每个数据点到其所属聚类中心的距离的平方和。

## 非参数化模型

## 决策树
决策树模型的阿才能书是分裂点的数量。每个分裂点选择的特征，特征的取值，训练前这些参数的数量通常是不固定的。

## KNN最近邻
在特征空间中，如果一个样本附近的k个最近(即特征空间中最邻近)样本的大多数属于某一个类别，则该样本也属于这个类别。  
KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。  
KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。